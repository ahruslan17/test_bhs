{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание выбранной архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура `Transformers` была предложена в статье `\"Attention is All You Need\"` (2017) от Google Brain. \n",
    "Основная идея - использование механизма внимания (attention), который позволяет моделям эффективно обрабатывать последовательности данных, таких как текст или временные ряды, без необходимости последовательной обработки данных, как это делается в рекуррентных нейронных сетях (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные компоненты Transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Механизм внимания (Attention Mechanism)`: Ключевой компонент, позволяющий модели сосредоточиться на различных частях входной последовательности для более эффективного предсказания. Он вычисляет веса внимания для каждого элемента последовательности, что позволяет модели учитывать контекст.\n",
    "- `Мультиголовной механизм внимания (Multi-Head Attention)`: Это расширение механизма внимания, которое позволяет модели фокусироваться на различных подпространствах внимания одновременно, что улучшает способность модели извлекать более богатую информацию из данных.\n",
    "- `Позиционное кодирование (Positional Encoding)`: Поскольку трансформеры обрабатывают весь вход одновременно, они не имеют встроенного понимания порядка последовательности. Позиционное кодирование добавляет информацию о положении элементов в последовательности, чтобы модель могла различать порядок слов.\n",
    "- `Энкодер и декодер (Encoder and Decoder)`: Архитектура трансформера обычно состоит из двух частей: энкодера и декодера. Энкодер обрабатывает входную последовательность и преобразует её в контекстные представления, которые затем используются декодером для генерации выходной последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Области применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка естественного языка (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Машинный перевод:` Применение трансформеров, таких как модели BERT и GPT, позволило значительно улучшить качество перевода текстов между различными языками. Например, модель BERT от Google достигла лучших результатов на многих стандартных наборах данных для задач машинного перевода.\n",
    "- `Анализ настроений:` Трансформеры используются для анализа настроений в текстах, таких как отзывы пользователей или комментарии в социальных сетях. Модели, такие как RoBERTa и XLNet, демонстрируют высокую точность в этой задаче.\n",
    "- `Генерация текста:` Модели GPT-3 и GPT-4 способны генерировать текст, который трудно отличить от текста, написанного человеком. Они находят применение в автоматической генерации контента, написании статей и создании диалоговых систем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Компьютерное зрение (Computer Vision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Классификация изображений`: Трансформеры начинают использоваться для задач классификации изображений, вытесняя традиционные CNN. Например, модель ViT (Vision Transformer) показала конкурентоспособные результаты на стандартных наборах данных, таких как ImageNet.\n",
    "- `Обнаружение объектов`: Модели, такие как DETR (Detection Transformer), используют трансформеры для более точного и эффективного обнаружения объектов на изображениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры:\n",
    "- Прогнозирование финансовых временных рядов других финансовых показателей. \n",
    "- Прогнозирование спроса на электроэнергию, анализ рядов потребления или сенсоров и т.д.\n",
    "- Обработка временных рядов ЭКГ и МРТ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Актуальность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NLP`\n",
    "В области обработки естественного языка трансформеры остаются на переднем крае исследований и разработок. Модели на основе трансформеров, такие как BERT, GPT и их производные, продолжают улучшаться и демонстрируют отличные результаты на многих задачах.\n",
    "\n",
    "`Компьютерное зрение`\n",
    "В компьютерном зрении трансформеры только начинают вытеснять CNN. Модели, такие как ViT и DETR, показывают, что трансформеры могут быть эффективными для задач зрения, и исследования в этом направлении активно продолжаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Обработка временных рядов`\n",
    "Для обработки временных рядов трансформеры также становятся все более популярными. Их способность учитывать долгосрочные зависимости и гибкость в работе с различными типами данных делает их актуальными для многих задач, связанных с временными рядами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). \"Attention is All You Need.\" Advances in Neural Information Processing Systems, 30. Link\n",
    "\n",
    "- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv preprint arXiv:1810.04805. Link\n",
    "\n",
    "- https://habr.com/ru/articles/486358/\n",
    "\n",
    "- https://habr.com/ru/companies/wunderfund/articles/592231/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
